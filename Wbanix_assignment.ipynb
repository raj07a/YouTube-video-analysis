{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install Whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0aLwKTBEXRw",
        "outputId": "e7357abd-52d2-4541-a404-c60e95feaa94"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Whisper\n",
            "  Downloading whisper-1.1.10.tar.gz (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from Whisper) (1.17.0)\n",
            "Building wheels for collected packages: Whisper\n",
            "  Building wheel for Whisper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Whisper: filename=whisper-1.1.10-py3-none-any.whl size=41120 sha256=a9ec63674d055f3efd0d7c15c057e6a6eef80a507fe0a66f296bbd0c2572d9f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/65/ee/4e6672aabfa486d3341a39a04f8f87c77e5156149299b5a7d0\n",
            "Successfully built Whisper\n",
            "Installing collected packages: Whisper\n",
            "Successfully installed Whisper-1.1.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpurHv6qvxKb",
        "outputId": "82c36e51-a316-45c1-9ac4-25c0d33127a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/57.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-15.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pytube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVKU2Fi1Mzr_",
        "outputId": "41efcc5c-a059-4a22-b44c-4239a228ac87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2025.2.19-py3-none-any.whl.metadata (171 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/171.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m163.8/171.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.9/171.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yt_dlp-2025.2.19-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yt-dlp\n",
            "Successfully installed yt-dlp-2025.2.19\n"
          ]
        }
      ],
      "source": [
        "pip install -U yt-dlp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEaVEqMjvdyu",
        "outputId": "648d320f-bc5d-4753-f2c1-f55ad991b8b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/800.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/800.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m798.7/800.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.5.1+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.6.0)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from triton>=2.0.0->openai-whisper) (3.17.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803373 sha256=31a28a7bbca8852228c049d5685027217373a43eeb283b7b65a03682a9b70ef4\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/f2/ce/6eb23db4091d026238ce76703bd66da60b969d70bcc81d5d3a\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20240930 tiktoken-0.9.0\n"
          ]
        }
      ],
      "source": [
        "pip install -U openai-whisper\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yt_dlp\n",
        "import whisper\n",
        "import nltk\n",
        "from pytube import YouTube\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import requests\n",
        "import time\n",
        "from textblob import TextBlob\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aD8h0gROkXH",
        "outputId": "51869c3e-9a0d-476d-e8b4-ad544df27a46"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "CuxIbs_iNA7j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e360f650-3e83-45e4-b3d4-16f97a5e200f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def get_subtitles(video_url):\n",
        "    \"\"\"Extracts subtitles using yt-dlp if available.\"\"\"\n",
        "    ydl_opts = {\n",
        "        'writesubtitles': True,\n",
        "        'writeautomaticsub': True,\n",
        "        'subtitleslangs': ['en'],\n",
        "        'skip_download': True,\n",
        "        'outtmpl': '%(id)s.%(ext)s'\n",
        "    }\n",
        "\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        try:\n",
        "            info = ydl.extract_info(video_url, download=False)\n",
        "            video_id = info.get('id')\n",
        "        except yt_dlp.utils.DownloadError as e:\n",
        "            if \"Video unavailable\" in str(e):\n",
        "                print(f\"Error: Video with URL '{video_url}' is geo-restricted or unavailable in your region.\")\n",
        "                return None\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "    subtitle_file = f\"{video_id}.en.vtt\"\n",
        "    return subtitle_file if os.path.exists(subtitle_file) else None\n",
        "\n",
        "def download_audio(video_url):\n",
        "    \"\"\"Downloads audio using yt-dlp.\"\"\"\n",
        "    ydl_opts = {\n",
        "        'format': 'bestaudio/best',\n",
        "        'outtmpl': 'audio.%(ext)s',\n",
        "        'postprocessors': [{\n",
        "            'key': 'FFmpegExtractAudio',\n",
        "            'preferredcodec': 'mp3',\n",
        "            'preferredquality': '192',\n",
        "        }],\n",
        "    }\n",
        "\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        ydl.download([video_url])\n",
        "\n",
        "    return \"audio.mp3\"\n",
        "\n",
        "def transcribe_audio(audio_path):\n",
        "    \"\"\"Converts speech to text using Whisper model.\"\"\"\n",
        "    model = whisper.load_model(\"base\")\n",
        "    result = model.transcribe(audio_path)\n",
        "    return result['text']\n",
        "\n",
        "def summarize_text(text, num_sentences=3):\n",
        "    \"\"\"Summarizes extracted text by selecting key sentences.\"\"\"\n",
        "    sentences = sent_tokenize(text)\n",
        "    return \" \".join(sentences[:num_sentences])\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    \"\"\"Performs sentiment analysis using TextBlob.\"\"\"\n",
        "    blob = TextBlob(text)\n",
        "    sentiment_score = blob.sentiment.polarity\n",
        "\n",
        "    if sentiment_score > 0.05:\n",
        "        return \"Positive\"\n",
        "    elif sentiment_score < -0.05:\n",
        "        return \"Negative\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "def process_video(video_url):\n",
        "    \"\"\"Main function to extract, summarize, and analyze sentiment from a YouTube video.\"\"\"\n",
        "    subtitle_file = get_subtitles(video_url)\n",
        "\n",
        "    if subtitle_file:\n",
        "        with open(subtitle_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read()\n",
        "    else:\n",
        "        audio_file = download_audio(video_url)\n",
        "        text = transcribe_audio(audio_file)\n",
        "        os.remove(audio_file)  # Cleanup\n",
        "\n",
        "    summary = summarize_text(text)\n",
        "    sentiment = analyze_sentiment(summary)\n",
        "\n",
        "    return summary, sentiment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_urls = [\n",
        "    \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",\n",
        "    \"https://www.youtube.com/watch?v=3JZ_D3ELwOQ\",\n",
        "    \"https://www.youtube.com/watch?v=Zi_XLOBDo_Y\",\n",
        "    \"https://www.youtube.com/watch?v=kJQP7kiw5Fk\",\n",
        "    \"https://www.youtube.com/watch?v=9bZkp7q19f0\"\n",
        "]\n",
        "\n",
        "for idx, video_url in enumerate(video_urls, 1):\n",
        "    summary, sentiment = process_video(video_url)\n",
        "    print(f\"**Video {idx}:**\")\n",
        "    print(f\"- **Summary:** \\\"{summary}\\\"\")\n",
        "    print(f\"- **Sentiment:** {sentiment}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irGhlw-qKFAl",
        "outputId": "e677242b-d8ff-4989-8929-ceeb1aa8e8d2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=dQw4w9WgXcQ\n",
            "[youtube] dQw4w9WgXcQ: Downloading webpage\n",
            "[youtube] dQw4w9WgXcQ: Downloading tv client config\n",
            "[youtube] dQw4w9WgXcQ: Downloading player f6e09c70\n",
            "[youtube] dQw4w9WgXcQ: Downloading tv player API JSON\n",
            "[youtube] dQw4w9WgXcQ: Downloading ios player API JSON\n",
            "[youtube] dQw4w9WgXcQ: Downloading m3u8 information\n",
            "[info] dQw4w9WgXcQ: Downloading subtitles: en\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=dQw4w9WgXcQ\n",
            "[youtube] dQw4w9WgXcQ: Downloading webpage\n",
            "[youtube] dQw4w9WgXcQ: Downloading tv client config\n",
            "[youtube] dQw4w9WgXcQ: Downloading player f6e09c70\n",
            "[youtube] dQw4w9WgXcQ: Downloading tv player API JSON\n",
            "[youtube] dQw4w9WgXcQ: Downloading ios player API JSON\n",
            "[youtube] dQw4w9WgXcQ: Downloading m3u8 information\n",
            "[info] dQw4w9WgXcQ: Downloading 1 format(s): 251\n",
            "[download] Destination: audio.webm\n",
            "[download] 100% of    3.28MiB in 00:00:00 at 26.01MiB/s  \n",
            "[ExtractAudio] Destination: audio.mp3\n",
            "Deleting original file audio.webm (pass -k to keep)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Video 1:**\n",
            "- **Summary:** \" Music There are no strangers to love You know the rules and something I've no commitments for the weekend of You wouldn't get this wrong any other guy I just wanna tell you how I feel I gotta make you understand Never gonna give you up Never gonna let you down Never gonna run around it He's hurt you Never gonna make you cry Never gonna say goodbye Never gonna tell him how I And hurt you We've known each other for so long Your heart's been aching but You're too shy to say it It's as if we've all known what's been going on We know the game I'm with Gonna play it And if you ask me how I feel I gotta make you understand Never gonna give you up Never gonna let you down Never gonna run around it He's hurt you Never gonna make you cry Never gonna say goodbye Never gonna tell him how I And hurt you Never gonna give you up Never gonna let you down Never gonna run around it He's hurt you Never gonna make you cry Never gonna say goodbye never gonna tell him how I And hurt you Never gonna give you up Never gonna give you up Never gonna give you up Never gonna give We've known each other for so long Your heart's been aching buzz You're just shy as to say It's half before, no one's been going on We know the game and we're gonna play I just wanna tell you how I feel It's gotta make you understand I'm gonna give you a, I'm gonna let you down I'm gonna run around it, he's meant to He's gonna make you cry I'm gonna save you by, I'm gonna tell him a lie He's meant to, he's gonna give you a, I'm gonna let you down I'm gonna run around it, he's meant to I'm gonna let you pass I'm gonna say goodbye I'm gonna say goodbye And praise you And I'm gonna give you a I'm gonna let you down I'm gonna fall over And you said too I'm gonna let you pass I'm gonna say goodbye I'm gonna say goodbye And praise you\"\n",
            "- **Sentiment:** Negative\n",
            "\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=3JZ_D3ELwOQ\n",
            "[youtube] 3JZ_D3ELwOQ: Downloading webpage\n",
            "[youtube] 3JZ_D3ELwOQ: Downloading tv client config\n",
            "[youtube] 3JZ_D3ELwOQ: Downloading player f6e09c70\n",
            "[youtube] 3JZ_D3ELwOQ: Downloading tv player API JSON\n",
            "[youtube] 3JZ_D3ELwOQ: Downloading ios player API JSON\n",
            "[youtube] 3JZ_D3ELwOQ: Downloading m3u8 information\n",
            "[info] 3JZ_D3ELwOQ: Downloading subtitles: en\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=3JZ_D3ELwOQ\n",
            "[youtube] 3JZ_D3ELwOQ: Downloading webpage\n",
            "[youtube] 3JZ_D3ELwOQ: Downloading tv client config\n",
            "[youtube] 3JZ_D3ELwOQ: Downloading player f6e09c70\n",
            "[youtube] 3JZ_D3ELwOQ: Downloading tv player API JSON\n",
            "[youtube] 3JZ_D3ELwOQ: Downloading ios player API JSON\n",
            "[youtube] 3JZ_D3ELwOQ: Downloading m3u8 information\n",
            "[info] 3JZ_D3ELwOQ: Downloading 1 format(s): 251\n",
            "[download] Destination: audio.webm\n",
            "[download] 100% of    3.39MiB in 00:00:00 at 20.55MiB/s  \n",
            "[ExtractAudio] Destination: audio.mp3\n",
            "Deleting original file audio.webm (pass -k to keep)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Video 2:**\n",
            "- **Summary:** \" Do you guys go to the gym? I go to the gym. But you already knew that.\"\n",
            "- **Sentiment:** Neutral\n",
            "\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=Zi_XLOBDo_Y\n",
            "[youtube] Zi_XLOBDo_Y: Downloading webpage\n",
            "[youtube] Zi_XLOBDo_Y: Downloading tv client config\n",
            "[youtube] Zi_XLOBDo_Y: Downloading player f6e09c70\n",
            "[youtube] Zi_XLOBDo_Y: Downloading tv player API JSON\n",
            "[youtube] Zi_XLOBDo_Y: Downloading ios player API JSON\n",
            "[youtube] Zi_XLOBDo_Y: Downloading m3u8 information\n",
            "[info] Zi_XLOBDo_Y: Downloading subtitles: en\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=Zi_XLOBDo_Y\n",
            "[youtube] Zi_XLOBDo_Y: Downloading webpage\n",
            "[youtube] Zi_XLOBDo_Y: Downloading tv client config\n",
            "[youtube] Zi_XLOBDo_Y: Downloading player f6e09c70\n",
            "[youtube] Zi_XLOBDo_Y: Downloading tv player API JSON\n",
            "[youtube] Zi_XLOBDo_Y: Downloading ios player API JSON\n",
            "[youtube] Zi_XLOBDo_Y: Downloading m3u8 information\n",
            "[info] Zi_XLOBDo_Y: Downloading 1 format(s): 251\n",
            "[download] Destination: audio.webm\n",
            "[download] 100% of    4.71MiB in 00:00:00 at 34.97MiB/s  \n",
            "[ExtractAudio] Destination: audio.mp3\n",
            "Deleting original file audio.webm (pass -k to keep)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Video 3:**\n",
            "- **Summary:** \" She was more like a beauty queen from a movie scene I shared her mind but don't do you mean I am the one Y'all good dance on the floor and around She said I am the one, good dance on the floor and around She told me her name is Billie Jean I said I am the one, good dance on the floor and around She told me her name is Billie Jean I shared her mind but don't do you mean I am the one Y'all good dance on the floor and around She told me her name is Billie Jean Where do I go ina wonder what you're doing I do but don't do you mean I am the one who you're not I do but don't do you mean I am the one I do but don't do you mean I am the one Turn that eye on the world, Turn that lower circle back, Turn that lower circle back Turn the lower circle back to frame For four to four to nine I was on the side But who can stand when he's in the manners? He's in the lands I'm going dance for the blow in round Something might throw advice Just remember to always think twice Don't think twice Shit told my baby there's two three That she looked at me and didn't shoot a photo But baby, prize eyes wouldn't like mine Don't I own? Go with dance, all over the floor Kill the wrong babe I keep on always told me Be careful what you do Don't go around bringing young girls' hearts Don't make me believe I'm not your claimant, still right behind me I just smell a sweet perfume I'm dancing at the front of the school I should vomit more The better genius that my love She just forget her who can't don't get all alone But the kid doesn't like my son No, no, no, no, no, no The better genius that my love She just forget her who can't don't get all alone But the kid doesn't like my son She just says I have a one But the kid doesn't like my son I can't keep She just doesn't like my one But the kid doesn't like my son No, no, no, no, no, no The better genius that my love She just forget her who can't don't get all alone But the baby, prize eyes wouldn't like mine Don't I own?\"\n",
            "- **Sentiment:** Positive\n",
            "\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=kJQP7kiw5Fk\n",
            "[youtube] kJQP7kiw5Fk: Downloading webpage\n",
            "[youtube] kJQP7kiw5Fk: Downloading tv client config\n",
            "[youtube] kJQP7kiw5Fk: Downloading player f6e09c70\n",
            "[youtube] kJQP7kiw5Fk: Downloading tv player API JSON\n",
            "[youtube] kJQP7kiw5Fk: Downloading ios player API JSON\n",
            "[youtube] kJQP7kiw5Fk: Downloading m3u8 information\n",
            "[info] kJQP7kiw5Fk: Downloading subtitles: en\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=kJQP7kiw5Fk\n",
            "[youtube] kJQP7kiw5Fk: Downloading webpage\n",
            "[youtube] kJQP7kiw5Fk: Downloading tv client config\n",
            "[youtube] kJQP7kiw5Fk: Downloading player f6e09c70\n",
            "[youtube] kJQP7kiw5Fk: Downloading tv player API JSON\n",
            "[youtube] kJQP7kiw5Fk: Downloading ios player API JSON\n",
            "[youtube] kJQP7kiw5Fk: Downloading m3u8 information\n",
            "[info] kJQP7kiw5Fk: Downloading 1 format(s): 251\n",
            "[download] Destination: audio.webm\n",
            "[download] 100% of    4.39MiB in 00:00:00 at 34.14MiB/s  \n",
            "[ExtractAudio] Destination: audio.mp3\n",
            "Deleting original file audio.webm (pass -k to keep)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Video 4:**\n",
            "- **Summary:** \" Eighth F particular character, Baby, get going super fast Fancy! Oh, oh no, oh no, oh no, oh, oh oh, oh-oh. Yo, si, sabes que ya llevo un rato mirándote.\"\n",
            "- **Sentiment:** Positive\n",
            "\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=9bZkp7q19f0\n",
            "[youtube] 9bZkp7q19f0: Downloading webpage\n",
            "[youtube] 9bZkp7q19f0: Downloading tv client config\n",
            "[youtube] 9bZkp7q19f0: Downloading player f6e09c70\n",
            "[youtube] 9bZkp7q19f0: Downloading tv player API JSON\n",
            "[youtube] 9bZkp7q19f0: Downloading ios player API JSON\n",
            "[youtube] 9bZkp7q19f0: Downloading m3u8 information\n",
            "[info] 9bZkp7q19f0: Downloading subtitles: en\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=9bZkp7q19f0\n",
            "[youtube] 9bZkp7q19f0: Downloading webpage\n",
            "[youtube] 9bZkp7q19f0: Downloading tv client config\n",
            "[youtube] 9bZkp7q19f0: Downloading player f6e09c70\n",
            "[youtube] 9bZkp7q19f0: Downloading tv player API JSON\n",
            "[youtube] 9bZkp7q19f0: Downloading ios player API JSON\n",
            "[youtube] 9bZkp7q19f0: Downloading m3u8 information\n",
            "[info] 9bZkp7q19f0: Downloading 1 format(s): 251\n",
            "[download] Destination: audio.webm\n",
            "[download] 100% of    3.81MiB in 00:00:00 at 28.06MiB/s  \n",
            "[ExtractAudio] Destination: audio.mp3\n",
            "Deleting original file audio.webm (pass -k to keep)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Video 5:**\n",
            "- **Summary:** \" BGM 호빤방담스던데 나 재는 따사로 온 인간적인 여자 커피한 잔에 여유로 하는 품격 있는 여자 밤이 어며 심장이 뜨거워지는 여자 그런 반 전 있는 여자 너는 선호해 나제는 너만큼 다사로운 그런 선호해 곧 비식이 도전해 원샷 때리는 선호해 밤이 오면 심장이 터져버린 선호해 그런 선호해 아름다워 사랑스러워 그래 너 해 그래 바로 너 해 아름다워 사랑스러워 그래 너 해 그래 바로 너 해 지금부터 그때까지 가볼까 그래 너 해 오빤 강남스타 강남스타 오빤 강남스타 강남스타 오빤 강남스타 에이 섹스 레이레 오빤 강남스타 에이 섹스 레이레 오빤 강남스타 청숙해 보이지만 볼 땐 너는 여자 이때다 싶으면 묶었던 머리 푸는 여자 가렸지만 왠 만한 노출 보다 야한 여자 그런 감각적인 여자 넌은 선호해 점점아 보이지만 볼 땐 너는 선호해 내가 되면 완전 미쳐 버리는 선호해 그녀 보다 사상이 올통벌통한 선호해 그런 선호해 아름다워 사랑스러워 그래 너 해 그래 바로 너 해 아름다워 사랑스러워 그래 너 해 그래 바로 너 해 지금부터 갈 때까지 가볼까 오빤 강남스타 강남스타 오빤 강남스타 강남스타 오빤 강남스타 에이 섹스 레이레 오빤 강남스타 에이 섹스 레이레 오빤 강남스타 비눈놈 그 위에 난은 넌 Baby Baby 난은 넌 좀 안은 넌 비눈놈 그 위에 난은 넌 Baby Baby 난은 넌 좀 안은 넌 좀 안은 넌 오빤 강남스타 에이 섹스 레이레 오빤 강남스타 에이 섹스 레이레 오빤 강남스타 오빤 강남스타\"\n",
            "- **Sentiment:** Neutral\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T5UOayolOU1H"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}